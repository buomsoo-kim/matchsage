{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f966bef9-860e-4e44-8467-5e5710ec545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425e2cd1-c073-44a0-bb9b-a2c33b16b957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19585102298848ae97043f7107d6d183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/rkim0927/Python/Data/VB/dating_suggestions.csv\")\n",
    "\n",
    "test_period_start, test_period_end = '2022-02-01', '2022-02-15'\n",
    "train_interaction = data[(data['created_at'] > '2021-11-30') & (data['created_at'] < test_period_start)]\n",
    "test_interaction = data[(data['created_at'] > test_period_start) & (data['created_at'] < test_period_end)]\n",
    "\n",
    "unique_users = list(set(pd.concat([train_interaction[\"source_id\"], train_interaction[\"user_id\"]])))\n",
    "test_interaction = test_interaction[(test_interaction[\"user_id\"].isin(unique_users)) & (test_interaction[\"source_id\"].isin(unique_users))]\n",
    "print(len(unique_users))\n",
    "\n",
    "user_to_idx, idx_to_user = {}, {}\n",
    "for i in tqdm(range(len(unique_users))):\n",
    "  user_to_idx[unique_users[i]] = i\n",
    "  idx_to_user[i] = unique_users[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbce7184-4f73-4e1c-aa69-f92abe899ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acf1977a32044c78a7845800d506471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_user = pd.read_csv('/Users/rkim0927/Python/Data/VB/Embeddings/embeddings_user_mf_rp.csv')\n",
    "embeddings_item = pd.read_csv('/Users/rkim0927/Python/Data/VB/Embeddings/embeddings_item_mf_rp.csv')\n",
    "embeddings_user.set_index(\"Unnamed: 0\", drop = True, inplace = True)\n",
    "embeddings_item.set_index(\"Unnamed: 0\", drop = True, inplace = True)\n",
    "\n",
    "user_feats, item_feats = [], []\n",
    "for i in tqdm(range(len(unique_users))):\n",
    "  u = unique_users[i]\n",
    "  if u in embeddings_user.index:\n",
    "    user_feats.append(embeddings_user.loc[u].values)\n",
    "    item_feats.append(embeddings_item.loc[u].values)\n",
    "  else:\n",
    "    user_feats.append(np.ones(50))\n",
    "    item_feats.append(np.ones(50))\n",
    "\n",
    "user_feats = np.array(user_feats)\n",
    "item_feats = np.array(item_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30dd99f8-945a-491e-a09b-9d558d25ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.SAGEConv(in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = HeteroDotProductPredictor()\n",
    "        # self.pred = MLPPredictor(out_features, 30, 1)\n",
    "    def forward(self, g, pos_g, neg_g, x1, x2, etype1, etype2):\n",
    "        h_src = self.sage(g, x1)     \n",
    "        h_dst = self.sage(g, x2)       \n",
    "        return self.pred(pos_g, h_src, h_dst, etype1), self.pred(neg_g, h_src,h_dst, etype2)\n",
    "\n",
    "class HeteroDotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h_src, h_dst, etype):\n",
    "        # h contains the node representations for each node type computed from\n",
    "        # the GNN defined in the previous section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h_src'] = h_src['user']\n",
    "            graph.ndata['h_dst'] = h_dst['user']\n",
    "            graph.apply_edges(fn.u_dot_v('h_src', 'h_dst', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score']\n",
    "\n",
    "def construct_negative_graph(graph, etype1, etype2, device, num_edges = 100000):\n",
    "    src, dst = graph.edges(etype=etype1)\n",
    "    indices = torch.LongTensor(np.random.choice(src.size(0), num_edges)).to(device)\n",
    "    pos_graph = dgl.heterograph(\n",
    "        {etype1: (src[indices], dst[indices])},\n",
    "        num_nodes_dict={ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes})\n",
    "    neg_src, neg_dst = graph.edges(etype=etype2)\n",
    "    indices = torch.LongTensor(np.random.choice(neg_src.size(0), num_edges)).to(device)\n",
    "    neg_graph = dgl.heterograph(\n",
    "        {etype2: (neg_src[indices], neg_dst[indices])},\n",
    "        num_nodes_dict={ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes})\n",
    "    return pos_graph, neg_graph\n",
    "\n",
    "def compute_loss(pos_score, neg_score, delta):\n",
    "    # Margin loss\n",
    "    n_edges = pos_score.shape[0]\n",
    "    return (delta - neg_score + pos_score).clamp(min=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06cdab7c-88ca-4305-80da-eeaac542c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "NODE_FEATURE_DIM = 50\n",
    "HIDDEN_FEATURE_DIM = 100\n",
    "EMBEDDING_DIM = 50\n",
    "MLP_HIDDEN_DIM = 30\n",
    "NUM_EPOCHS = 1000\n",
    "k1, k2 = 100000, 30000\n",
    "alpha = .8\n",
    "\n",
    "# device = \"mps\" if torch.backends.mps.is_built() else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5693fe16-3133-4088-8eb9-fcda21902ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'user': 21392},\n",
       "      num_edges={('user', 'accepts', 'user'): 834278, ('user', 'rejects', 'user'): 4153220},\n",
       "      metagraph=[('user', 'user', 'accepts'), ('user', 'user', 'rejects')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(777)\n",
    "\n",
    "train_accepted = train_interaction[train_interaction[\"accepted\"] == 1]\n",
    "train_rejected = train_interaction[train_interaction[\"accepted\"] == 0]\n",
    "acc_src, acc_dst = torch.LongTensor(train_accepted['user_id'].map(user_to_idx).values).to(device), torch.LongTensor(train_accepted['source_id'].map(user_to_idx).values).to(device)\n",
    "rej_src, rej_dst = torch.LongTensor(train_rejected['user_id'].map(user_to_idx).values).to(device), torch.LongTensor(train_rejected['source_id'].map(user_to_idx).values).to(device)\n",
    "\n",
    "graph = dgl.heterograph({\n",
    "    ('user', 'accepts', 'user'): (acc_src, acc_dst),\n",
    "    ('user', 'rejects', 'user'): (rej_src, rej_dst),\n",
    "},  num_nodes_dict = {\"user\": len(idx_to_user)}).to(device)\n",
    "\n",
    "# graph.nodes['user'].data['src_feature'] = torch.randn(graph.nodes(\"user\").size(0), NODE_FEATURE_DIM).to(device)\n",
    "# graph.nodes['user'].data['dst_feature'] = torch.randn(graph.nodes(\"user\").size(0), NODE_FEATURE_DIM).to(device)\n",
    "\n",
    "graph.nodes['user'].data['src_feature'] = torch.FloatTensor(user_feats).to(device)\n",
    "graph.nodes['user'].data['dst_feature'] = torch.FloatTensor(item_feats).to(device)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "070326d1-26d6-4f63-94f3-c5eeee70f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(NODE_FEATURE_DIM, HIDDEN_FEATURE_DIM, EMBEDDING_DIM, graph.etypes).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "src_feats, dst_feats = {\"user\": graph.nodes['user'].data['src_feature']} , {\"user\": graph.nodes['user'].data['dst_feature']}\n",
    "etype_1, etype_2 = ('user', 'accepts', 'user'),  ('user', 'rejects', 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff0a10d4-7b4e-468d-ad13-a928819227b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.540107727050781\n",
      "101 0.5375457406044006\n",
      "201 0.4386550486087799\n",
      "301 0.40113264322280884\n",
      "401 0.37233319878578186\n",
      "501 0.34544888138771057\n",
      "601 0.3359788656234741\n",
      "701 0.32296472787857056\n",
      "801 0.3087318241596222\n",
      "901 0.29816579818725586\n",
      "CPU times: user 12min 19s, sys: 1min 16s, total: 13min 35s\n",
      "Wall time: 12min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    pos_graph, negative_graph = construct_negative_graph(graph, etype_1, etype_2, device, k1)\n",
    "    pos_score, neg_score = model(graph, pos_graph,  negative_graph, src_feats, dst_feats, etype_1, etype_2)\n",
    "    loss = compute_loss(pos_score, neg_score, 1)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 100 == 0:\n",
    "      print(epoch+1, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c56a55ec-1981-4cf4-b037-9f9cbb6f19fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5392fd3e66d04231bdbddecc0515fe50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_src, embeddings_dst = model.sage(graph, src_feats)['user'].detach().cpu().numpy(), model.sage(graph, dst_feats)['user'].detach().cpu().numpy()\n",
    "embeddings_src, embeddings_dst = pd.DataFrame(embeddings_src, index = [x[1] for x in idx_to_user.items()]), pd.DataFrame(embeddings_dst, index = [x[1] for x in idx_to_user.items()])\n",
    "\n",
    "src_to_embedding, dst_to_embedding = dict(), dict()\n",
    "\n",
    "for i in tqdm(range(len(embeddings_src))):\n",
    "  src_to_embedding[embeddings_src.index[i]] = embeddings_src.iloc[i].values\n",
    "  dst_to_embedding[embeddings_dst.index[i]] = embeddings_dst.iloc[i].values\n",
    "\n",
    "embeddings_src.to_csv('/Users/rkim0927/Python/Data/VB/Embeddings/embeddings_user_matchsage.csv')\n",
    "embeddings_dst.to_csv('/Users/rkim0927/Python/Data/VB/Embeddings/embeddings_item_matchsage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d989b30e-5302-4788-869f-1e402d7aa952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>262146</th>\n",
       "      <td>-0.247022</td>\n",
       "      <td>-0.585324</td>\n",
       "      <td>0.278355</td>\n",
       "      <td>0.977576</td>\n",
       "      <td>-0.163174</td>\n",
       "      <td>-0.149333</td>\n",
       "      <td>0.403666</td>\n",
       "      <td>-0.390639</td>\n",
       "      <td>-0.680525</td>\n",
       "      <td>-0.156883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407454</td>\n",
       "      <td>0.752831</td>\n",
       "      <td>-0.312989</td>\n",
       "      <td>0.467062</td>\n",
       "      <td>-0.176985</td>\n",
       "      <td>0.059873</td>\n",
       "      <td>-0.637069</td>\n",
       "      <td>-0.173985</td>\n",
       "      <td>0.361946</td>\n",
       "      <td>-0.644119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262148</th>\n",
       "      <td>-0.384095</td>\n",
       "      <td>0.053177</td>\n",
       "      <td>-0.289479</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>0.738665</td>\n",
       "      <td>-0.146694</td>\n",
       "      <td>0.907702</td>\n",
       "      <td>0.080897</td>\n",
       "      <td>-0.097276</td>\n",
       "      <td>-0.384564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123898</td>\n",
       "      <td>0.409229</td>\n",
       "      <td>0.353526</td>\n",
       "      <td>-0.080921</td>\n",
       "      <td>-0.013004</td>\n",
       "      <td>0.048866</td>\n",
       "      <td>-0.733887</td>\n",
       "      <td>-0.161099</td>\n",
       "      <td>0.156275</td>\n",
       "      <td>0.329346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262149</th>\n",
       "      <td>-0.289618</td>\n",
       "      <td>-0.337384</td>\n",
       "      <td>0.102390</td>\n",
       "      <td>0.013831</td>\n",
       "      <td>0.232124</td>\n",
       "      <td>0.133435</td>\n",
       "      <td>0.583250</td>\n",
       "      <td>-0.023239</td>\n",
       "      <td>-0.526302</td>\n",
       "      <td>-0.112118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341626</td>\n",
       "      <td>0.104950</td>\n",
       "      <td>0.211426</td>\n",
       "      <td>0.144730</td>\n",
       "      <td>0.130463</td>\n",
       "      <td>0.100896</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>-0.274512</td>\n",
       "      <td>0.252849</td>\n",
       "      <td>-0.126965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262150</th>\n",
       "      <td>0.122197</td>\n",
       "      <td>-0.993227</td>\n",
       "      <td>0.185879</td>\n",
       "      <td>0.237607</td>\n",
       "      <td>-0.149140</td>\n",
       "      <td>-0.687928</td>\n",
       "      <td>0.437555</td>\n",
       "      <td>0.309256</td>\n",
       "      <td>-0.616605</td>\n",
       "      <td>0.277166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400098</td>\n",
       "      <td>-0.206687</td>\n",
       "      <td>0.053693</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>-0.293778</td>\n",
       "      <td>-0.139575</td>\n",
       "      <td>-0.892943</td>\n",
       "      <td>0.542541</td>\n",
       "      <td>0.386484</td>\n",
       "      <td>0.122656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262151</th>\n",
       "      <td>0.076501</td>\n",
       "      <td>-0.542083</td>\n",
       "      <td>-0.270100</td>\n",
       "      <td>-0.023784</td>\n",
       "      <td>0.291219</td>\n",
       "      <td>-0.047542</td>\n",
       "      <td>0.270747</td>\n",
       "      <td>-0.035526</td>\n",
       "      <td>-0.445480</td>\n",
       "      <td>0.152017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403062</td>\n",
       "      <td>-0.034659</td>\n",
       "      <td>-0.034196</td>\n",
       "      <td>0.244297</td>\n",
       "      <td>-0.081065</td>\n",
       "      <td>-0.018950</td>\n",
       "      <td>-0.262636</td>\n",
       "      <td>0.292451</td>\n",
       "      <td>-0.133327</td>\n",
       "      <td>0.154459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262139</th>\n",
       "      <td>-0.356279</td>\n",
       "      <td>0.101586</td>\n",
       "      <td>-0.224972</td>\n",
       "      <td>0.202832</td>\n",
       "      <td>0.815259</td>\n",
       "      <td>-0.079751</td>\n",
       "      <td>0.770300</td>\n",
       "      <td>0.278017</td>\n",
       "      <td>0.185470</td>\n",
       "      <td>-0.250616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219999</td>\n",
       "      <td>0.262693</td>\n",
       "      <td>0.267423</td>\n",
       "      <td>-0.045921</td>\n",
       "      <td>-0.353057</td>\n",
       "      <td>0.129104</td>\n",
       "      <td>-0.791793</td>\n",
       "      <td>-0.090569</td>\n",
       "      <td>-0.094683</td>\n",
       "      <td>0.403881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262140</th>\n",
       "      <td>-0.172082</td>\n",
       "      <td>-0.328460</td>\n",
       "      <td>-0.406324</td>\n",
       "      <td>0.698802</td>\n",
       "      <td>0.648844</td>\n",
       "      <td>0.094804</td>\n",
       "      <td>0.352157</td>\n",
       "      <td>0.038808</td>\n",
       "      <td>-0.443557</td>\n",
       "      <td>-0.256785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172910</td>\n",
       "      <td>0.076585</td>\n",
       "      <td>-0.721370</td>\n",
       "      <td>-0.521600</td>\n",
       "      <td>-0.057247</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>-0.397897</td>\n",
       "      <td>-0.091874</td>\n",
       "      <td>0.108164</td>\n",
       "      <td>-0.021169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262141</th>\n",
       "      <td>-0.494139</td>\n",
       "      <td>-0.371082</td>\n",
       "      <td>-0.330752</td>\n",
       "      <td>0.311240</td>\n",
       "      <td>-0.230299</td>\n",
       "      <td>-0.309441</td>\n",
       "      <td>0.285026</td>\n",
       "      <td>0.422687</td>\n",
       "      <td>-0.307018</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383108</td>\n",
       "      <td>0.477025</td>\n",
       "      <td>0.387020</td>\n",
       "      <td>0.066760</td>\n",
       "      <td>0.594881</td>\n",
       "      <td>0.369909</td>\n",
       "      <td>-0.462010</td>\n",
       "      <td>-0.087332</td>\n",
       "      <td>-0.583248</td>\n",
       "      <td>-0.068032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262142</th>\n",
       "      <td>-0.332772</td>\n",
       "      <td>-0.103500</td>\n",
       "      <td>-0.557458</td>\n",
       "      <td>0.532374</td>\n",
       "      <td>0.518756</td>\n",
       "      <td>0.051194</td>\n",
       "      <td>0.777940</td>\n",
       "      <td>-0.095660</td>\n",
       "      <td>-0.081268</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202976</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.032891</td>\n",
       "      <td>-0.074116</td>\n",
       "      <td>-0.029475</td>\n",
       "      <td>0.059299</td>\n",
       "      <td>0.061885</td>\n",
       "      <td>0.046481</td>\n",
       "      <td>-0.154132</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262143</th>\n",
       "      <td>-0.391310</td>\n",
       "      <td>-0.594534</td>\n",
       "      <td>-0.376690</td>\n",
       "      <td>0.112844</td>\n",
       "      <td>-0.036053</td>\n",
       "      <td>-0.155231</td>\n",
       "      <td>0.788796</td>\n",
       "      <td>-0.029802</td>\n",
       "      <td>-0.332287</td>\n",
       "      <td>-0.389571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173451</td>\n",
       "      <td>0.091004</td>\n",
       "      <td>0.277203</td>\n",
       "      <td>0.445843</td>\n",
       "      <td>-0.172346</td>\n",
       "      <td>0.630445</td>\n",
       "      <td>-0.344856</td>\n",
       "      <td>-0.318339</td>\n",
       "      <td>-0.444155</td>\n",
       "      <td>0.056839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21392 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "262146 -0.247022 -0.585324  0.278355  0.977576 -0.163174 -0.149333  0.403666   \n",
       "262148 -0.384095  0.053177 -0.289479  0.169507  0.738665 -0.146694  0.907702   \n",
       "262149 -0.289618 -0.337384  0.102390  0.013831  0.232124  0.133435  0.583250   \n",
       "262150  0.122197 -0.993227  0.185879  0.237607 -0.149140 -0.687928  0.437555   \n",
       "262151  0.076501 -0.542083 -0.270100 -0.023784  0.291219 -0.047542  0.270747   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "262139 -0.356279  0.101586 -0.224972  0.202832  0.815259 -0.079751  0.770300   \n",
       "262140 -0.172082 -0.328460 -0.406324  0.698802  0.648844  0.094804  0.352157   \n",
       "262141 -0.494139 -0.371082 -0.330752  0.311240 -0.230299 -0.309441  0.285026   \n",
       "262142 -0.332772 -0.103500 -0.557458  0.532374  0.518756  0.051194  0.777940   \n",
       "262143 -0.391310 -0.594534 -0.376690  0.112844 -0.036053 -0.155231  0.788796   \n",
       "\n",
       "              7         8         9   ...        40        41        42  \\\n",
       "262146 -0.390639 -0.680525 -0.156883  ...  0.407454  0.752831 -0.312989   \n",
       "262148  0.080897 -0.097276 -0.384564  ...  0.123898  0.409229  0.353526   \n",
       "262149 -0.023239 -0.526302 -0.112118  ...  0.341626  0.104950  0.211426   \n",
       "262150  0.309256 -0.616605  0.277166  ...  0.400098 -0.206687  0.053693   \n",
       "262151 -0.035526 -0.445480  0.152017  ...  0.403062 -0.034659 -0.034196   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "262139  0.278017  0.185470 -0.250616  ...  0.219999  0.262693  0.267423   \n",
       "262140  0.038808 -0.443557 -0.256785  ...  0.172910  0.076585 -0.721370   \n",
       "262141  0.422687 -0.307018  0.023818  ...  0.383108  0.477025  0.387020   \n",
       "262142 -0.095660 -0.081268  0.202454  ...  0.202976  0.007067  0.032891   \n",
       "262143 -0.029802 -0.332287 -0.389571  ...  0.173451  0.091004  0.277203   \n",
       "\n",
       "              43        44        45        46        47        48        49  \n",
       "262146  0.467062 -0.176985  0.059873 -0.637069 -0.173985  0.361946 -0.644119  \n",
       "262148 -0.080921 -0.013004  0.048866 -0.733887 -0.161099  0.156275  0.329346  \n",
       "262149  0.144730  0.130463  0.100896 -0.016758 -0.274512  0.252849 -0.126965  \n",
       "262150  0.090392 -0.293778 -0.139575 -0.892943  0.542541  0.386484  0.122656  \n",
       "262151  0.244297 -0.081065 -0.018950 -0.262636  0.292451 -0.133327  0.154459  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "262139 -0.045921 -0.353057  0.129104 -0.791793 -0.090569 -0.094683  0.403881  \n",
       "262140 -0.521600 -0.057247  0.024184 -0.397897 -0.091874  0.108164 -0.021169  \n",
       "262141  0.066760  0.594881  0.369909 -0.462010 -0.087332 -0.583248 -0.068032  \n",
       "262142 -0.074116 -0.029475  0.059299  0.061885  0.046481 -0.154132  0.204300  \n",
       "262143  0.445843 -0.172346  0.630445 -0.344856 -0.318339 -0.444155  0.056839  \n",
       "\n",
       "[21392 rows x 50 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_src"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
